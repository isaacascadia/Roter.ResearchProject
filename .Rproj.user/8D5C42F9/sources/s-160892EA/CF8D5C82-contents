
# this script file contains the custom functions used in 3.simulation.testing.R
# Function 1 simulates a dataset, 
# Function 2 performs Jaccard-based cluster analysis with heatmap & dendrogram,
# Function 3 performs a PCA with a scatter plot of PC1 & PC2 and a scree plot


#================== 1. data simulation function ================================

# This function simulates a dataset for three populations: alp(ine), 
# sub(alpine), and val(ley). n.vars controls the number of different species
# (or variables) used in the data set. For each population, three things can be
# controlled: the number of observations from the population (e.g. alp.n), the 
# mean of eachs pecies' abundance or presence in the population's samples 
# (e.g. alp.m), and the standard deviation of species abundances or presences 
# (e.g. alp.sd). The function uses fixed seedsfor pseudrorandom number 
# generation to ensure reproducibility. The output is saved to "data.raw".


# defining the function fsimulate()

# the objects for means and SDs are a string of numbers equal in length to the
# number of variables
fsimulate <- function(name, n.vars,           # dataset name for files/figs
                      alp.n, alp.m, alp.sd,   # alpine sample characteristics
                      sub.n, sub.m, sub.sd,   # subalpine sample characteristics
                      val.n, val.m, val.sd){  # valley sample characteristics
  
  # make a matrix to fill with data that has as many rows as there are samples
  # and as many columns as there are variables
  simul.data <- matrix(nrow = (alp.n + sub.n + val.n), ncol = n.vars)
  
  # give the rows (samples) numbered names based on the population they're from
  rownames(simul.data) <- c(paste("alp", 1:alp.n, sep = ""),
                            paste("sub", 1:sub.n, sep = ""),
                            paste("val", 1:val.n, sep = ""))
  
  # name the species/variables
  colnames(simul.data) <- paste("species", 1:n.vars, sep = "")
  
  # one seed for each variable
  seeds <- 1:n.vars * 1234
  
  # the heart of the function! For each variable, generate a string of random
  # values from a normal distribution based on the parameters for each 
  # population, such that the values generated align with their associated pop.
  for(i in 1:n.vars){
    set.seed(seeds[i])
    alp.values <- rnorm(n = alp.n, m = alp.m[i], sd = alp.sd[i])  # alp values
    set.seed(seeds[i] + 1)
    sub.values <- rnorm(n = sub.n, m = sub.m[i], sd = sub.sd[i])  # sub values
    set.seed(seeds[i] + 2)
    val.values <- rnorm(n = val.n, m = val.m[i], sd = val.sd[i])  # val values
    
    # enter the data into the appropriate species column in the data matrix
    simul.data[,i] <- c(alp.values, sub.values, val.values)
  } # end of data simulation loop
  
  # saving simulated data to "data.raw" folder
  write.csv(simul.data, paste(path.data.raw, name, 
                              ".data.csv", sep = ""))
 
  # return simulated data to user
  return(simul.data)
  
} # end of simulation function





#================== 2. Cluster analysis, heatmap and dendrogram function =======

# This function calculates Jaccard index of similarity for a presence/absence
# matrix with observations as rows and species as columns. The outputted Jaccard
# matrix is saved to "data.output". The function generates a heatmap of the 
# matrix and a dendrogram based on a Jaccard distance matrix generated by 
# vegdist(). These visuals are saved to "figures". Due to difficulty with the 
# pheatmap() function, this function does not display heatmaps in the RStudio 
# graphics device.


fjaccard <- function(binary.matrix, ji.name){

  # how many observations are there?
  n.plots <- nrow(binary.matrix)
  
  # how many combinations of observations are there?
  combs <- combinations(n = nrow(binary.matrix), r = 2)
  
  # add to the list of combinations all the plots compared with themselves
  # (e.g. 1-1, 2-2, etc.)
  combs <- rbind(combs, matrix(rep(x = 1:n.plots, 2), 
                               nrow = n.plots, ncol = 2))
  
  # create empty, square matrix with side length equal to the # of observations
  j.output <- matrix(data = NA, 
                     nrow = n.plots, 
                     ncol = n.plots)
  
  # give the empty matrix the row and column names of the input matrix
  rownames(j.output) <- rownames(binary.matrix)
  colnames(j.output) <- rownames(binary.matrix)
  
  
  # this loop is the heart of the function! It calculates Jaccard similarity for 
  # every possible combination of samples in the dataset.
  for(i in 1:nrow(combs)){
    
    
    # which rows (plots) are being compared?
    n1 <- combs[i,1]  # first plot 
    n2 <- combs[i,2]  # second plot
    
    # which species are present in each row/plot?
    trees.i <- which(binary.matrix[n1,] == 1)
    trees.j <- which(binary.matrix[n2,] == 1)
    
    # if no species are present in one of the plots, assign the value 0 to the 
    # output instead of attempting to divide by 0 (for obvious reasions, R 
    # doesn't like to divide by 0)
    if(length(c(trees.i, trees.j)) == 0){
      j.output[n1, n2] <- 0
      j.output[n2, n1] <- 0
      
    } else {  # if no plots contains 0 species, continue with the JI calculation
      
      # For each pair, it calculates Jaccard index by answering these questions:
      # "What number of species are present inboth plot i and plot j? 
      # How many unique species are present between the two plots? 
      # What is the proportion of shared species (intersection) to total number 
      # of species (union)?" 
      # The function then inputs this information into two cells in the output 
      # matrix using the ranked order of the two plots as coordinates.
      
      j.output[n1, n2] <- length(intersect(trees.i, trees.j)) / 
        unique(c(trees.i, trees.j)) %>% length()
      
      # input the value into the mirrored coordinate for a symmetrical plot
      j.output[n2, n1] <- length(intersect(trees.i, trees.j)) / 
        unique(c(trees.i, trees.j)) %>% length()
    }  # end else statement
    
  }  # end of jaccard index loop
  
  
  # my JI function doesn't output the "dist"-type object needed for hierarchical
  # clustering, so I'm forced to use a function from the vegan package to create
  # a "dist" matrix based on jaccard index. However, it uses distance, not 
  # similarity, so requires some attention.
  vd <- vegdist(binary.matrix, method = "jaccard", upper = F)
  
  # turning dist object into a hierarchical cluster using UPGMA (averaging)
  hc <- hclust(vd, method = "average")
  
  # turning the hierarchical cluster object into a dendrogram, ready to plot!
  hcd <- as.dendrogram(hc)
  
  
  # now we go into saving our figures.
  # pheatmap() doesn't like displaying heatmaps if the graphics device is 
  # currently in use, so this if() statement wipes the graphics device if it's
  # in use
  if(dev.cur() > 1){  # is the graphics device on?
    dev.off()         # turn it off!
  }                   # end if() statement
  
  # begin saving of heatmap and dendrogram to "figures" folder"
  pdf(paste(path.figures, gsub(" ", "", ji.name), 
            ".heatmap.dendro.pdf", sep = ""),
      width = 5, height = 5)
  
  # display heatmap to be saved. rows and columns are not clustered, so the 
  # heatmap directly displays the matrix outputted from the JI loop.
  pheatmap(j.output, main =  paste(ji.name, "JI scores"), 
           legend_labels = c("JI Score"), 
           cluster_rows = F, cluster_cols = F, las = 1)
  
  # plot dendrogram object based on jaccard distance and UPGMA clustering
  plot(hcd, type = "rectangle", leaflab = "perpendicular", 
       nodePar = list(lab.cex = 0.8, pch = NA), 
       xlab = "Plots", ylab = "Jaccard distance", cex.lab = 1.05,
       main = paste(ji.name, "dendrogram"))
  
  dev.off()  # finish saving plots
  
  # save output matrix of Jaccard similarities as a .csv in "data.output" folder
  write.csv(j.output, paste(path.data.output, gsub(" ", "", ji.name), 
                            ".jaccard.csv", sep = ""))
  
  # display output matrix of Jaccard similarities to the user
  return(j.output)
  
}  # end jaccard function


#================== 3. PCA, scatterplot and scree plot function ================

# this function performs a PCA on an inputted matrix (with observations as rows
# and variables as columns), then shows the user (and saves to the "figures" 
# folder) a scatterplot of the first 2 principal components and a "scree plot" 
# (percent of variation explained by each principal component). It then shows  
# the user the variables with the highest loading scores for PC1. The principal 
# component axes and all loading scores are saved to "data.output".


# samps.vars is a matrix class object
fpca <- function(samps.vars, pca.name){  # pca.name: char type > plot/pdf titles
  
  # expects samples to be rows, and variables to be columns 
  pca <- prcomp(samps.vars, scale = TRUE)  # returns x (PCs)
  
  # what is the variation covered by each PC?
  pca.var <- pca$sdev^2
  
  # what percent of the total variation is covered by each PC?
  pca.var.per <- round(pca.var/sum(pca.var) * 100, 1)
  
  
  # begin saving of pca plots to "figures" folder
  pdf(paste(path.figures, gsub(" ", "", pca.name), ".pca.plots.pdf", sep = ""),
      width = 5, height = 5) 
  
  # What does the plot of our first two principal components look like?
  plot(pca$x[,1], pca$x[,2], 
       xlab = paste("PC1 - ", pca.var.per[1], "%", sep=""), 
       ylab = paste("PC2 - ", pca.var.per[2], "%", sep=""),
       main = paste("PCs of", pca.name, sep = " "))
  
  # Scree plot of variation distribution among PCs
  barplot(pca.var.per, 
          main = paste("Scree Plot of", pca.name, sep = " "), 
          xlab = "Principal Component", cex.names = 0.9, las = 1,
          ylab = "Percent Variation", names.arg = colnames(pca$x),
          ylim = c(0, max(pca.var.per) + 10))   
  
  dev.off()  # finish saving plots
  
  
  # display pc plots in RStudio graphics window
  
  # Scree plot of variation distribution among PCs
  barplot(pca.var.per, 
          main = paste("Scree Plot of", pca.name, sep = " "), 
          xlab = "Principal Component", cex.names = 0.9, las = 1,
          ylab = "Percent Variation", names.arg = colnames(pca$x),
          ylim = c(0, max(pca.var.per) + 10))  
  
  # What does the plot of our first two principal components look like?
  plot(pca$x[,1], pca$x[,2], 
       xlab = paste("PC1 - ", pca.var.per[1], "%", sep=""), 
       ylab = paste("PC2 - ", pca.var.per[2], "%", sep=""),
       main = paste("PCs of", pca.name, sep = " "))
  
  
  # loading scores goodness
  loading.scores <- pca$rotation[,1]   # display loading scores for PC1
  var.scores <- abs(loading.scores)                     # loading magnitudes
  var.score.ranked <- sort(var.scores, decreasing = T)  # ordered loading scores
  
  # this block of if() statements determines the appropriate number of loading 
  # scores to display to the user based on the total number of variables.
  if(ncol(samps.vars) <= 5){top.vars <- 
    names(var.score.ranked[1:ncol(samps.vars)])}
  if(ncol(samps.vars) >= 6 && ncol(samps.vars) <= 10){top.vars <- 
    names(var.score.ranked[1:5])}
  if(ncol(samps.vars) >= 11){top.vars <- names(var.score.ranked[1:10])}
  
  # save loading scores as .csv
  write.csv(pca$rotation, paste(path.data.output, gsub(" ", "", pca.name), 
                                ".loading.csv", sep = ""))
  
  # save principle component values as .csv
  write.csv(pca$x, paste(path.data.output, gsub(" ", "", pca.name), 
                         ".pc.values.csv", sep = ""))
  
  # showing the user the largest loading scores
  print("highest loading scores")   # names of most important variables
  return(pca$rotation[top.vars,1])  # names, scores of most important variables
  
  
}  # end of pca function
